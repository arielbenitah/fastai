{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
      "(60000, 784) (60000,) (10000, 784) (10000,) 0 9\n",
      "60000 784 10\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1] * x_train.shape[1]))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1] * x_test.shape[1]))\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape, y_train.min(), y_train.max())\n",
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "print(n, m, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x - m) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33.318421449829934, 78.56748998339798)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_test  = normalize(x_test,  train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.064638490070051e-17, 0.9999999999999998)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in order to run the kernel, 5 x 5 - see bellow cells, I reshape \n",
    "# # into 60000 x 28 x 28 x 1, the last dimension is because we have only one channel\n",
    "# x_train = tf.reshape(x_train, [x_train.shape[0], 28, 28])[..., None]\n",
    "# x_test  = tf.reshape(x_test , [x_test.shape[0], 28, 28])[..., None]\n",
    "# x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.imshow(tf.squeeze(x_train[0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden layer\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, nh, n_out):        \n",
    "        super().__init__()\n",
    "        self.lrs = [tf.keras.layers.Dense(nh), tf.keras.layers.ReLU(), tf.keras.layers.Dense(n_out)]\n",
    "        \n",
    "    def call(self, x):\n",
    "        for l in self.lrs: x = l(x)\n",
    "        return x\n",
    "    \n",
    "model = Model(nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0102 13:09:31.903322 139782540642048 base_layer.py:1814] Layer model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# activate the model\n",
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x7f20db716198>,\n",
       " <tensorflow.python.keras.layers.advanced_activations.ReLU at 0x7f20db716470>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f20db7165f8>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  39250     \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): return tf.math.exp(x)/tf.math.reduce_sum(tf.math.exp(x), 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return tf.math.log(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=65, shape=(10,), dtype=float32, numpy=\n",
       "array([-1.2655183, -1.7178358, -4.6140556, -1.8788285, -2.149867 ,\n",
       "       -2.792309 , -3.8598835, -3.129559 , -4.4795275, -2.1049902],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first element in y_train[:3] above is the element we want (which is 5):\n",
    "sm_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=73, shape=(), dtype=float32, numpy=-2.149867>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we want\n",
    "sm_pred[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-2.792309 -2.122984 -3.45119 ], shape=(3,), dtype=float32)\n",
      "tf.Tensor([-2.792309 -2.122984 -3.45119 ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# so instead of calculating -sum(x * log(x)) and do \"waste time\" calculating stuff like 0 * log(q)\n",
    "# Jeremy presents a solution in the notebook: in pytorch : sm_pred[[0, 1, 2], [5, 0, 4]]\n",
    "# In tensorflow this can be done as follow:\n",
    "# you want to collect ONLY relevant ground true probabilities (==1.0), \n",
    "# so in the example above, we need to gather for the first row of sm_pred, position 5\n",
    "# from the 2 row position 0\n",
    "# from the 3rd row position 4\n",
    "# in tf this can be done as follow:\n",
    "# tf.gather_nd(sm_pred, [[0, 5], [1, 0], [2, 4]])\n",
    "print(tf.gather_nd(sm_pred, np.array([[0, 5], [1, 0], [2, 4]])))\n",
    "# or in a more formalized way:\n",
    "print(tf.gather_nd(sm_pred, np.stack((np.arange(len(y_train[:3])), y_train[:3])).transpose()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so instead of summing unnecessary elements, there is a better way (as shown above) of\n",
    "# calculating the negative log likelihood (nll):\n",
    "def nll(input, target): \n",
    "    \n",
    "    # rearange target such that it looks as above:\n",
    "    pick = np.stack((np.arange(len(target)), target)).transpose()\n",
    "                    \n",
    "    # nll\n",
    "    return -tf.math.reduce_mean(tf.gather_nd(input, pick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.7888277, shape=(), dtype=float32)\n",
      "tf.Tensor(2.7888277, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(nll(sm_pred, y_train[:3]))\n",
    "print(-tf.math.reduce_mean(tf.gather_nd(sm_pred, np.array([[0, 5], [1, 0], [2, 4]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=92, shape=(), dtype=float32, numpy=2.7140043>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_pred, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since it's the log of the softmax, it can be rewritten as:\n",
    "def log_softmax(x): return x - tf.math.log(tf.math.reduce_sum(tf.math.exp(x), 1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=101, shape=(10,), dtype=float32, numpy=\n",
       "array([-1.2655182, -1.7178357, -4.6140556, -1.8788284, -2.149867 ,\n",
       "       -2.7923088, -3.8598835, -3.1295588, -4.479527 , -2.10499  ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[0] # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log sum exp trick:\n",
    "# high floating points number are not accurate, so the logsumexp trick comes to the rescue\n",
    "# see https://course.fast.ai/videos/?lesson=9 at 40 min\n",
    "def logsumexp(x):\n",
    "    # find the maximum:\n",
    "    m = tf.math.reduce_max(x)\n",
    "    \n",
    "    # it's just a trick that extract the maximum out of the sum:\n",
    "    return m + tf.math.log(tf.math.reduce_sum(tf.math.exp(x-m), 1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=109, shape=(60000, 1), dtype=float32, numpy=\n",
       "array([[3.0857718],\n",
       "       [2.619653 ],\n",
       "       [3.1994472],\n",
       "       ...,\n",
       "       [3.4174674],\n",
       "       [2.4986951],\n",
       "       [2.9030917]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my logsumexp\n",
    "logsumexp(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=121, shape=(60000, 1), dtype=float32, numpy=\n",
       "array([[3.0857716],\n",
       "       [2.6196532],\n",
       "       [3.1994472],\n",
       "       ...,\n",
       "       [3.4174674],\n",
       "       [2.4986951],\n",
       "       [2.903092 ]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf logsumexp\n",
    "tf.math.reduce_logsumexp(pred, 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - tf.math.reduce_logsumexp(x, 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.7140038, shape=(), dtype=float32)\n",
      "tf.Tensor(2.7140043, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(nll(log_softmax(pred), y_train))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.7140043, shape=(), dtype=float32)\n",
      "tf.Tensor(2.7140043, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# in tensorflow, nll is a combination of mean + softmax + cross entropy.\n",
    "# softmax + cross entropy are combined into one function called\n",
    "# tf.nn.softmax_cross_entropy_with_logits(y_true, y_pred)\n",
    "# HOWEVER, y_train must be one hot encoded and to calculate the loss\n",
    "# we need to calculate the mean on the result on tf.nn.softmax_cross_entropy_with_logits\n",
    "# interesting discussion on stackoverflow:\n",
    "# https://stackoverflow.com/questions/34240703/what-is-logits-softmax-and-softmax-cross-entropy-with-logits\n",
    "# Using tf:\n",
    "print(tf.math.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.one_hot(y_train, 10), pred)))\n",
    "# Using Keras:\n",
    "print(tf.math.reduce_mean(tf.keras.losses.categorical_crossentropy(tf.one_hot(y_train, 10), softmax(pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is repeating over the following steps:\n",
    "1. get the outputof the model on a batch of inputs\n",
    "2. compare the output to the labels we have and compute a loss\n",
    "3. calculate the gradients of the loss with respect to every parameter of the model\n",
    "4. update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here target MUST be one hot encoded\n",
    "# corss entropy loss\n",
    "def cross_entropy(targets, predictions):\n",
    "    return tf.math.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(targets, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the target MUST NOT BE one hot encoded\n",
    "def accuracy(targets, predictions):\n",
    "    y_predictions = tf.cast(tf.argmax(predictions, axis=1), dtype=tf.float32)\n",
    "    y_targets = tf.cast(targets, dtype=tf.float32)\n",
    "    return tf.math.reduce_mean(tf.cast(tf.math.equal( y_predictions, y_targets) , dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=219, shape=(10,), dtype=float32, numpy=\n",
       " array([ 1.8202534 ,  1.3679359 , -1.528284  ,  1.2069432 ,  0.93590456,\n",
       "         0.29346266, -0.7741119 , -0.0437873 , -1.3937556 ,  0.98078144],\n",
       "       dtype=float32)>, TensorShape([64, 10]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "xb = x_train[:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=258, shape=(), dtype=float32, numpy=2.7845428>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[:bs]\n",
    "cross_entropy(tf.one_hot(yb, 10), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.375"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy percentage\n",
    "accuracy(yb, preds).numpy()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5 # learning rate\n",
    "epochs = 1 # how many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs + 1):        \n",
    "            start_i = i*bs\n",
    "            end_i   = start_i+bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # calculate loss\n",
    "                loss = cross_entropy(tf.one_hot(yb, 10), model(xb))\n",
    "\n",
    "                # calculate gradient            \n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "                # update\n",
    "                for i, parameters in enumerate(model.trainable_variables):                \n",
    "                    parameters.assign_sub(lr * gradients[i])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15762433, 95.3125)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the loss is improved as well as the accuracy\n",
    "fit()\n",
    "cross_entropy(tf.one_hot(yb, 10), model(xb)).numpy(), accuracy(yb, model(xb)).numpy()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model above can be written as follow using sequential:\n",
    "# Sequential inherits from Model!!\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(nh), tf.keras.layers.ReLU(), tf.keras.layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will replace the lines 18-19 in the fit() function above\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit2():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs + 1):        \n",
    "            start_i = i*bs\n",
    "            end_i   = start_i+bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # calculate loss\n",
    "                loss = cross_entropy(tf.one_hot(yb, 10), model(xb))\n",
    "\n",
    "                # calculate gradient            \n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "                # update\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0102 13:09:42.418771 139782540642048 base_layer.py:1814] Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.31112832, 92.1875)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit2()\n",
    "cross_entropy(tf.one_hot(yb, 10), model(xb)).numpy(), accuracy(yb, model(xb)).numpy()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lenght of dataset\n",
    "nt, nv = len(list(train_ds)), len(list(valid_ds))\n",
    "nt, nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 784]), TensorShape([64]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract one batch from dataset\n",
    "image, label = next(iter(train_ds))\n",
    "image.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(image[0], [28, 28]), cmap='gray')\n",
    "print('label = ', label[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# another way is to loop on the dataset and use take:\n",
    "for image, label in train_ds.take(1):\n",
    "    plt.imshow(tf.reshape(image[0], [28, 28]), cmap='gray')\n",
    "    print('label = ', label[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, the fit function becomes:\n",
    "def fit3():\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # loop on the dataset\n",
    "        for xb, yb in train_ds:            \n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                # calculate preditions:\n",
    "                predictions = model(xb)\n",
    "\n",
    "                # calculate loss\n",
    "                loss = cross_entropy(tf.one_hot(yb, 10), predictions)\n",
    "\n",
    "                # calculate gradient            \n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "                # update\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19535127, 95.3125)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit3()\n",
    "cross_entropy(tf.one_hot(yb, 10), model(xb)).numpy(), accuracy(yb, model(xb)).numpy()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, the fit function becomes:\n",
    "def fit4():\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # loop on the training dataset\n",
    "        for xb, yb in train_ds:            \n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                # calculate preditions:\n",
    "                predictions = model(xb)\n",
    "\n",
    "                # calculate loss\n",
    "                loss = cross_entropy(tf.one_hot(yb, 10), predictions)\n",
    "\n",
    "                # calculate gradient            \n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "                # update\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # loop on the validation dataset\n",
    "        tot_loss, tot_acc = 0., 0.    \n",
    "        for xb, yb in valid_ds:\n",
    "            \n",
    "            # predictions\n",
    "            predictions = model(xb)\n",
    "            \n",
    "            # total loss\n",
    "            tot_loss += cross_entropy(tf.one_hot(yb, 10), predictions).numpy()\n",
    "            \n",
    "            # total accuracy\n",
    "            tot_acc += accuracy(yb, predictions).numpy()*100\n",
    "       \n",
    "        # total number of elements in validation set\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "        \n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.31415770974627155 91.90883757961784\n",
      "1 0.3365966368312384 91.49084394904459\n",
      "2 0.3152571506916907 92.43630573248407\n",
      "3 0.4686549094618316 88.11703821656052\n",
      "4 0.5612629761646508 86.38535031847134\n"
     ]
    }
   ],
   "source": [
    "loss, acc = fit4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
