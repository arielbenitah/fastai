{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://course.fast.ai/videos/?lesson=9 @1:23:34\n",
    "# Callbacks\n",
    "# keep track of metrics: progress bar animation\n",
    "# hyper params scheduling \n",
    "# regularization techniques\n",
    "# tensorboards\n",
    "# mixed precision training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
      "(60000, 784) (60000,) (10000, 784) (10000,) 0 9\n",
      "60000 784 10\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1] * x_train.shape[1]))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1] * x_test.shape[1]))\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape, y_train.min(), y_train.max())\n",
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "print(n, m, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x - m) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33.318421449829934, 78.56748998339798)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_test  = normalize(x_test,  train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.064638490070051e-17, 0.9999999999999998)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 50\n",
    "lr = 0.5\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, nh, n_out):        \n",
    "        super().__init__()\n",
    "        self.lrs = [tf.keras.layers.Dense(nh), tf.keras.layers.ReLU(), tf.keras.layers.Dense(n_out)]\n",
    "        \n",
    "    def call(self, x):\n",
    "        for l in self.lrs: x = l(x)\n",
    "        return x\n",
    "    \n",
    "# corss entropy loss\n",
    "def cross_entropy(targets, predictions):\n",
    "    return tf.math.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(targets, predictions))\n",
    "\n",
    "# here the target MUST NOT BE one hot encoded\n",
    "def accuracy(targets, predictions):\n",
    "    y_predictions = tf.cast(tf.argmax(predictions, axis=1), dtype=tf.float32)\n",
    "    y_targets = tf.cast(targets, dtype=tf.float32)\n",
    "    return tf.math.reduce_mean(tf.cast(tf.math.equal( y_predictions, y_targets) , dtype=tf.float32))\n",
    "\n",
    "# class cross_entropy():\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "#     def call(self, targets, predictions):\n",
    "#         return tf.math.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(targets, predictions))\n",
    "\n",
    "    \n",
    "model = Model(nh, 10)\n",
    "loss_function = cross_entropy\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0119 14:52:00.020627 140153097033472 base_layer.py:1814] Layer model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# activate the model\n",
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing Your Model into a Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea is to pack your model into something that take\n",
    "# 1. the model \n",
    "# 2. the optimization method \n",
    "# 3. the loss function amd 4. the data\n",
    "class Learner():\n",
    "    def __init__(self, model, optimizer, loss_function, data):\n",
    "        self.model, self.optimizer, self.loss_function, self.data = model, optimizer, loss_function, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data as well can be packed into a DataBunch containing \n",
    "# 1. the training data\n",
    "# 2. the validation data\n",
    "# 3. the classes from 0 to 9\n",
    "# in a future version it can be interesting to get a subsample of the full data\n",
    "# for training purposes\n",
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, classes=None):\n",
    "        self.train_dl, self.valid_dl, self.classes = train_dl, valid_dl, classes\n",
    "        self.train_samples, self.valid_samples =  len(list(self.train_dl)), len(list(self.valid_dl))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, y, classes):\n",
    "    return x, tf.one_hot(y, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<MapDataset shapes: ((None, 784), (None, 10)), types: (tf.float64, tf.float32)>,\n",
       " <MapDataset shapes: ((None, 784), (None, 10)), types: (tf.float64, tf.float32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the dataset in tensorflow\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.map(lambda x, y: one_hot(x, y, classes=c))\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE)\n",
    "valid_ds = valid_ds.map(lambda x, y: one_hot(x, y, classes=c))\n",
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_ds, valid_ds, classes=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack the full model into a learner\n",
    "learn = Learner(model=model, optimizer=optimizer, loss_function=loss_function, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "def fit(epochs, learn):\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in learn.data.train_dl:\n",
    "            with tf.GradientTape() as tape:                    \n",
    "                loss = learn.loss_function(yb, learn.model(xb))                \n",
    "                gradients = tape.gradient(loss, learn.model.trainable_variables)\n",
    "                learn.optimizer.apply_gradients(zip(gradients, learn.model.trainable_variables))\n",
    "                \n",
    "        tot_loss, tot_acc = 0., 0.    \n",
    "        for xb, yb in learn.data.valid_dl:\n",
    "            tot_loss += learn.loss_function(yb, learn.model(xb)).numpy()\n",
    "            tot_acc  += accuracy(tf.argmax(yb, axis=-1), learn.model(xb)).numpy()*100\n",
    "                \n",
    "        print(epoch, tot_loss/learn.data.valid_samples, tot_acc/learn.data.valid_samples)\n",
    "    return tot_loss/learn.data.valid_samples, tot_acc/learn.data.valid_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.42360708575434747 88.93312101910828\n"
     ]
    }
   ],
   "source": [
    "loss, acc = fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now how to fix this and insert callbacks: metrics and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "\n",
    "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
    "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
    "def camel2snake(name):\n",
    "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
    "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
    "\n",
    "class Callback():\n",
    "    _order=0\n",
    "    def set_runner(self, run): self.run=run\n",
    "    def __getattr__(self, k): return getattr(self.run, k)\n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):        \n",
    "        self.run.n_epochs=0.\n",
    "        self.run.n_iter=0\n",
    "    \n",
    "    def after_batch(self):        \n",
    "        if not self.in_train: return\n",
    "        self.run.n_epochs += 1./self.iters\n",
    "        self.run.n_iter   += 1\n",
    "        \n",
    "    def begin_training(self):        \n",
    "        self.run.n_epochs=self.epoch\n",
    "#         self.model.train()\n",
    "        self.run.in_train=True\n",
    "\n",
    "    def begin_validate(self):        \n",
    "#         self.model.eval()\n",
    "        self.run.in_train=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_eval_callback'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbname = 'TrainEvalCallback'\n",
    "camel2snake(cbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_eval'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainEvalCallback().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import *\n",
    "\n",
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        cbs = listify(cbs)        \n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):       return self.learn.optimizer\n",
    "    @property\n",
    "    def model(self):           return self.learn.model\n",
    "    @property\n",
    "    def loss_function(self):   return self.learn.loss_function\n",
    "    @property\n",
    "    def data(self):            return self.learn.data\n",
    "\n",
    "    def one_batch(self, xb, yb):        \n",
    "        # if watch_accessed_variables is True, the variables will be watched for gradients computation\n",
    "        # if watch_accessed_variables is False, no variables are watched for further gradients computation        \n",
    "        with tf.GradientTape(watch_accessed_variables=self.in_train) as tape:            \n",
    "            self.xb,self.yb = xb,yb\n",
    "            if self('begin_batch'): return\n",
    "            # if training is True, this enables to calculate batchnorm and dropout\n",
    "            # else batchnorm and dropout are disabled\n",
    "            self.pred = self.model(self.xb, training=self.in_train)\n",
    "            if self('after_pred'): return\n",
    "            self.loss = self.loss_function(self.yb, self.pred)            \n",
    "            if self('after_loss') or not self.in_train: return\n",
    "            gradients = tape.gradient(self.loss, self.model.trainable_variables)\n",
    "            if self('after_gradients'): return\n",
    "            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "            if self('after_step'): return        \n",
    "\n",
    "    def all_batches(self, dl):        \n",
    "        self.iters = len(list(dl))\n",
    "        for xb,yb in dl:\n",
    "            if self.stop: break\n",
    "            self.one_batch(xb, yb)\n",
    "            self('after_batch')\n",
    "        self.stop=False\n",
    "\n",
    "    def fit(self, epochs, learn):          \n",
    "        self.epochs,self.learn = epochs,learn                \n",
    "        try:  \n",
    "            # set runner for all callbacks\n",
    "            for cb in self.cbs: \n",
    "                cb.set_runner(self)     \n",
    "                \n",
    "            # on begin fit what to do?\n",
    "            if self('begin_fit'): \n",
    "                return            \n",
    "            for epoch in range(epochs):                \n",
    "                self.epoch = epoch\n",
    "                \n",
    "                # on begin epoch what to do?\n",
    "                if not self('begin_epoch'):                     \n",
    "                    self.all_batches(self.data.train_dl)\n",
    "                \n",
    "                # on begin validate what to do?                \n",
    "                if not self('begin_validate'): \n",
    "                    self.all_batches(self.data.valid_dl)\n",
    "                    \n",
    "                if self('after_epoch'): break\n",
    "            \n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            f = getattr(cb, cb_name, None)\n",
    "            if f and f(): return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAcc():\n",
    "    def __init__(self, metrics, in_train):\n",
    "        self.metrics, self.in_train = metrics, in_train\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.tot_loss, self.tot_acc = 0., 0.\n",
    "        \n",
    "    def calculate(self, run):      \n",
    "#         set_trace()\n",
    "        self.tot_loss += run.loss.numpy()\n",
    "        self.tot_acc  += self.metrics(tf.argmax(run.yb, axis=-1), run.pred).numpy()*100\n",
    "    \n",
    "#     def print_loss_acc(self, run):\n",
    "#         print(self.tot_loss / run.data.valid_samples, self.tot_acc / run.data.valid_samples)\n",
    "        \n",
    "class LossAccCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.epoch = 0\n",
    "        self.valid_lossacc = LossAcc(metrics, in_train=False)\n",
    "        self.train_lossacc = LossAcc(metrics, in_train=True)\n",
    "        \n",
    "    def begin_epoch(self):        \n",
    "        self.train_lossacc.reset()\n",
    "        self.run.in_train=True\n",
    "    \n",
    "    def after_loss(self):\n",
    "        if self.run.in_train:\n",
    "            self.train_lossacc.calculate(self.run)\n",
    "        else:\n",
    "            self.valid_lossacc.calculate(self.run)\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        self.print_train(self.run)\n",
    "        self.print_valid(self.run)\n",
    "        self.epoch += 1\n",
    "        \n",
    "    def begin_validate(self):        \n",
    "        self.valid_lossacc.reset()\n",
    "        self.run.in_train=False\n",
    "        \n",
    "    def print_valid(self, run):\n",
    "        print('Valid: ', \n",
    "              self.epoch, \n",
    "              self.valid_lossacc.tot_loss / run.data.valid_samples, \n",
    "              self.valid_lossacc.tot_acc / run.data.valid_samples)\n",
    "        \n",
    "    def print_train(self, run):\n",
    "        print('Train: ', \n",
    "              self.epoch, \n",
    "              self.train_lossacc.tot_loss / run.data.train_samples, \n",
    "              self.train_lossacc.tot_acc / run.data.train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = LossAccCallback(accuracy)\n",
    "run = Runner(cbs=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-50-26b1802b9fd0>(25)begin_epoch()\n",
      "-> self.train_lossacc.reset()\n",
      "(Pdb) l\n",
      " 20  \t        self.valid_lossacc = LossAcc(metrics, in_train=False)\n",
      " 21  \t        self.train_lossacc = LossAcc(metrics, in_train=True)\n",
      " 22  \t\n",
      " 23  \t    def begin_epoch(self):\n",
      " 24  \t        set_trace()\n",
      " 25  ->\t        self.train_lossacc.reset()\n",
      " 26  \t        self.run.in_train=True\n",
      " 27  \t\n",
      " 28  \t    def after_loss(self):\n",
      " 29  \t        if self.run.in_train:\n",
      " 30  \t            self.train_lossacc.calculate(self.run)\n",
      "(Pdb) self.train\n",
      "*** AttributeError: 'Runner' object has no attribute 'train'\n",
      "(Pdb) self.train_lossacc\n",
      "<__main__.LossAcc object at 0x7f771fb03860>\n",
      "(Pdb) self.run\n",
      "<__main__.Runner object at 0x7f771fb03d30>\n",
      "(Pdb) self.run.in_train\n",
      "*** AttributeError: 'Runner' object has no attribute 'in_train'\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-82a289ee3567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-357a2d7d29ed>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, learn)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# on begin epoch what to do?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-357a2d7d29ed>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, cb_name)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-26b1802b9fd0>\u001b[0m in \u001b[0;36mbegin_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_lossacc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-26b1802b9fd0>\u001b[0m in \u001b[0;36mbegin_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_lossacc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
